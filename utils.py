import torch
from tqdm import tqdm
import numpy as np
import pandas as pd
import argparse
from torch.utils.data import Dataset
import os

medians_mean = torch.tensor([
    51.4309, 65.1785, -12.0250, 54.8314,
    82.8575, 86.3554, 81.6232, 56.3015
]).unsqueeze(1)

medians_std = torch.tensor([
    97.9711, 119.0020, 123.9675, 215.6933,
    193.0294, 190.7409, 171.7255, 139.4443
]).unsqueeze(1)


def parse_args():
    parser = argparse.ArgumentParser(
        fromfile_prefix_chars='@', description="ECG Normalizing Flow trainer"
    )
    parser.add_argument(
        "--n_channels", default=8, type=int, help="number of ECG channels"
    )
    parser.add_argument(
        "--n_scales",
        default=3,
        type=int,
        help="number of model's hierarchical scales"
    )
    parser.add_argument(
        "--n_steps",
        default=2,
        type=int,
        help="number of flow's steps per scale"
    )
    parser.add_argument(
        "--n_latent_steps",
        default=2,
        type=int,
        help="number of flow's steps applied to pixels left at each scale"
    )
    parser.add_argument(
        "--n_samples",
        default=10,
        type=int,
        help="number of samples generated by the model every 10th epoch"
    )
    parser.add_argument(
        "--neg_slope",
        default=0.1,
        type=float,
        help="negative slope of model's Leaky ReLU"
    )
    parser.add_argument(
        "--actnorm_eps", default=1e-6, type=float, help="ActNorm's epsilon"
    )
    parser.add_argument(
        "--batch", default=512, type=int, help="batch size"
    )
    parser.add_argument(
        "--lr", default=1e-4, type=float, help="learning rate"
    )
    parser.add_argument(
        "--val_frac",
        default=0.1,
        type=float,
        help="fraction of the dataset to be left as validation set"
    )
    parser.add_argument(
        "--n_epochs", default=100, type=int, help="number of epochs"
    )
    parser.add_argument(
        "--no_normalization", action="store_true", help="don't normalize"
    )
    parser.add_argument(
        "--n_workers",
        default=4,
        type=int,
        help="number of workers used in dataloader"
    )
    parser.add_argument(
        "--annot_path",
        type=str,
        help="path to the annotations file, and when not specified, full" +
             "dataset is loaded from pickle file",
    )
    parser.add_argument(
        "--stats_path",
        default="./",
        type=str,
        help="path to the directory where all data related to the training " +
             "status is stored",
    )
    parser.add_argument("path", type=str, help="path to ECGs' directory")
    parsed_args = parser.parse_args()
    return parsed_args


class ECGDatasetFromFile(Dataset):
    def __init__(self, annotations_df, ecgs_dir, n_scales, n_channels,
                 size=-1, mean=None, std=None):
        if size == -1:
            self.ecg_labels = annotations_df
        else:
            self.ecg_labels = annotations_df.head(size).copy()
        self.ecgs_dir = ecgs_dir
        self.mean = mean
        self.std = std
        self.dim_red = 2 ** (2 * n_scales - 1)
        self.n_channels = n_channels

    def __len__(self):
        return len(self.ecg_labels)

    def __getitem__(self, idx):
        ecg_path = os.path.join(self.ecgs_dir, self.ecg_labels.iloc[idx, 0])
        ecg = np.loadtxt(ecg_path, delimiter=' ')
        ecg = torch.tensor(ecg).transpose(1, 0)
        if self.mean is not None:
            ecg = ecg - self.mean
        if self.std is not None:
            ecg = ecg / self.std
        if ecg.shape[1] % self.dim_red != 0:
            trim_size = ecg.shape[1] % self.dim_red
            trim_l = trim_size // 2
            trim_r = ecg.shape[1] - (trim_size - trim_l)
            ecg = ecg[:, trim_l:trim_r]
        return ecg[:self.n_channels, :]


class ECGDataset(Dataset):
    def __init__(self, dataset_tensor, n_scales, n_channels, size=-1, mean=None,
                 std=None):
        if size == -1:
            self.ecgs = dataset_tensor
        else:
            self.ecgs = dataset_tensor[:size, :, :]

        if mean is not None:
            self.ecgs = self.ecgs - mean
        if std is not None:
            self.ecgs = self.ecgs / std

        dim_red = 2 ** (2 * n_scales - 1)
        signal_len = self.ecgs.shape[2]
        if signal_len % dim_red != 0:
            trim_size = signal_len % dim_red
            trim_l = trim_size // 2
            trim_r = signal_len - (trim_size - trim_l)
            self.ecgs = self.ecgs[:, :, trim_l:trim_r]

        self.ecgs = self.ecgs[:, :n_channels, :]

    def __len__(self):
        return self.ecgs.shape[0]

    def __getitem__(self, idx):
        return self.ecgs[idx]


def dataset_mean_std(dl, n_channels):
    mean = torch.zeros(n_channels, dtype=torch.float64)
    n_pixels = 0
    with tqdm(dl) as pbar:
        pbar.set_description("Calculating mean")
        for x in pbar:
            n_pixels += x.shape[0] * x.shape[2]
            x = x.transpose(1, 0).reshape(8, -1).sum(dim=1)
            mean += x
    mean = mean / n_pixels

    mean = mean.unsqueeze(1)
    std = torch.zeros(n_channels, dtype=torch.float64)
    with tqdm(dl) as pbar:
        pbar.set_description("Calculating std")
        for x in pbar:
            x = ((x - mean) ** 2).transpose(1, 0).reshape(8, -1).sum(dim=1)
            std += x
    mean = mean.squeeze(1)
    std = torch.sqrt(std / n_pixels)

    return mean, std


def get_pickle_datasets(args):
    dataset = torch.load(args.path)
    print(dataset)
    dataset_size = dataset.shape[0]
    perm = torch.randperm(dataset.shape[0])
    val_dataset_size = int(dataset_size * args.val_frac)
    train_dataset = ECGDataset(
        dataset[perm[val_dataset_size:]],
        args.n_scales,
        args.n_channels,
        mean=None if args.no_normalization else medians_mean,
        std=None if args.no_normalization else medians_std
    )
    val_dataset = ECGDataset(
        dataset[perm[:val_dataset_size]],
        args.n_scales,
        args.n_channels,
        mean=None if args.no_normalization else medians_mean,
        std=None if args.no_normalization else medians_std
    )
    return train_dataset, val_dataset


def train_val_split(annot_path, val_frac):
    annot_df = pd.read_csv(annot_path)
    val_annot = annot_df.sample(frac=val_frac)
    train_annot = annot_df.drop(val_annot.index)
    return train_annot.reset_index(drop=True), val_annot.reset_index(drop=True)


def get_datasets_from_file(args):
    train_annot, val_annot = train_val_split(
        args.annot_path, args.val_frac
    )
    train_dataset = ECGDatasetFromFile(
        train_annot,
        args.path,
        args.n_scales,
        args.n_channels,
        mean=None if args.no_normalization else medians_mean,
        std=None if args.no_normalization else medians_std
    )
    val_dataset = ECGDatasetFromFile(
        val_annot,
        args.path,
        args.n_scales,
        args.n_channels,
        mean=None if args.no_normalization else medians_mean,
        std=None if args.no_normalization else medians_std
    )
    return train_dataset, val_dataset


def sample_from_model(
        net,
        distribution,
        sample_size,
        epoch,
        device,
        n_channels,
        n_samples,
        stats_path
):
    for i in range(n_samples):
        z = distribution.sample([sample_size]).flatten().unsqueeze(0)
        with torch.no_grad():
            x = net.reverse(z.double().to(device))
        x = x.cpu().detach().numpy().reshape((n_channels, -1)).transpose(1, 0)
        np.savetxt(
            f'{stats_path}/training-samples/{epoch}-{i}.asc', x, delimiter=' '
        )
